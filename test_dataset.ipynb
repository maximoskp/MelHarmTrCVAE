{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_utils import SeparatedMelHarmMarkovDataset\n",
    "import os\n",
    "import numpy as np\n",
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, GCTRootPCTokenizer, \\\n",
    "    GCTSymbolTokenizer, GCTRootTypeTokenizer, MelodyPitchTokenizer, \\\n",
    "    MergedMelHarmTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/ssd2/maximos/data/hooktheory_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordSymbolTokenizer = ChordSymbolTokenizer.from_pretrained('saved_tokenizers/ChordSymbolTokenizer')\n",
    "rootTypeTokenizer = RootTypeTokenizer.from_pretrained('saved_tokenizers/RootTypeTokenizer')\n",
    "pitchClassTokenizer = PitchClassTokenizer.from_pretrained('saved_tokenizers/PitchClassTokenizer')\n",
    "rootPCTokenizer = RootPCTokenizer.from_pretrained('saved_tokenizers/RootPCTokenizer')\n",
    "melodyPitchTokenizer = MelodyPitchTokenizer.from_pretrained('saved_tokenizers/MelodyPitchTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_chordSymbolTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, chordSymbolTokenizer)\n",
    "m_rootTypeTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootTypeTokenizer)\n",
    "m_pitchClassTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, pitchClassTokenizer)\n",
    "m_rootPCTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootPCTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChordSymbolTokenizer\n"
     ]
    }
   ],
   "source": [
    "print(m_chordSymbolTokenizer.harmony_tokenizer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = m_chordSymbolTokenizer\n",
    "tokenizer_name = 'ChordSymbolTokenizer'\n",
    "\n",
    "dataset = SeparatedMelHarmMarkovDataset(root_dir, tokenizer, max_length=512, num_bars=64)\n",
    "# Data collator for BART\n",
    "def create_data_collator(tokenizer, model):\n",
    "    return DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = BartConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    forced_eos_token_id=tokenizer.eos_token_id,\n",
    "    max_position_embeddings=512,\n",
    "    encoder_layers=8,\n",
    "    encoder_attention_heads=8,\n",
    "    encoder_ffn_dim=512,\n",
    "    decoder_layers=8,\n",
    "    decoder_attention_heads=8,\n",
    "    decoder_ffn_dim=512,\n",
    "    d_model=512,\n",
    "    encoder_layerdrop=0.3,\n",
    "    decoder_layerdrop=0.3,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "bart = BartForConditionalGeneration(bart_config)\n",
    "\n",
    "bart_path = 'saved_models/bart/' + tokenizer_name + '/' + tokenizer_name + '.pt'\n",
    "if device == 'cpu':\n",
    "    checkpoint = torch.load(bart_path, map_location=\"cpu\", weights_only=True)\n",
    "else:\n",
    "    checkpoint = torch.load(bart_path, weights_only=True)\n",
    "bart.load_state_dict(checkpoint)\n",
    "\n",
    "bart.to(device)\n",
    "bart.eval()\n",
    "\n",
    "bart_encoder, bart_decoder = bart.get_encoder(), bart.get_decoder()\n",
    "bart_encoder.to(device)\n",
    "bart_decoder.to(device)\n",
    "\n",
    "# Freeze BART parameters\n",
    "for param in bart_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in bart_encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = create_data_collator(tokenizer, model=bart)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/.local/lib/python3.11/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "/home/maximos/.local/lib/python3.11/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'][5].sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 348, 348])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "def build_batch_graphs(markov_matrices):\n",
    "    \"\"\"\n",
    "    Converts a batch of Markov transition matrices into a single batched PyTorch Geometric graph.\n",
    "\n",
    "    Args:\n",
    "        markov_matrices (torch.Tensor): (batch_size, num_nodes, num_nodes) tensor\n",
    "\n",
    "    Returns:\n",
    "        batch_graph (Batch): Batched PyG graph containing all transition matrices\n",
    "        node_indices (torch.Tensor): (batch_size,) tensor containing a node index per sample\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, _ = markov_matrices.shape\n",
    "    graphs = []\n",
    "    node_indices = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        # Extract nonzero entries (source, target) where transition probability > 0\n",
    "        source_nodes, target_nodes = torch.nonzero(markov_matrices[b], as_tuple=True)\n",
    "        edge_probs = markov_matrices[b][source_nodes, target_nodes]  # Extract transition probabilities\n",
    "\n",
    "        # Create edge_index\n",
    "        edge_index = torch.stack([source_nodes, target_nodes], dim=0)  # Shape (2, num_edges)\n",
    "        \n",
    "        # Create graph data object\n",
    "        graph = Data(edge_index=edge_index, edge_attr=edge_probs, num_nodes=num_nodes)\n",
    "        graphs.append(graph)\n",
    "\n",
    "        # Select a random node to condition on (or use a rule)\n",
    "        node_indices.append(torch.randint(0, num_nodes, (1,)))\n",
    "\n",
    "    # Batch all graphs into a single PyG Batch object\n",
    "    batch_graph = Batch.from_data_list(graphs)\n",
    "    node_indices = torch.cat(node_indices)  # Shape (batch_size,)\n",
    "\n",
    "    return batch_graph, node_indices\n",
    "# end build_batch_graphs\n",
    "\n",
    "def compute_loss(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Compute VAE loss (Reconstruction Loss + KL Divergence).\n",
    "    \n",
    "    Args:\n",
    "        recon_x (torch.Tensor): Reconstructed sequences (batch_size, seq_len, transformer_dim)\n",
    "        x (torch.Tensor): Ground truth sequences (batch_size, seq_len, transformer_dim)\n",
    "        mu (torch.Tensor): Mean of latent distribution (batch_size, latent_dim)\n",
    "        logvar (torch.Tensor): Log variance of latent distribution (batch_size, latent_dim)\n",
    "    \n",
    "    Returns:\n",
    "        loss (torch.Tensor): Combined loss\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "# end compute_loss\n",
    "\n",
    "class GraphConditioningModule(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim, use_attention=False):\n",
    "        \"\"\"\n",
    "        Graph-based conditioning module for extracting node embeddings as condition vectors.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Hidden dimension of GNN layers\n",
    "            out_dim (int): Dimension of the conditioning vector\n",
    "            use_attention (bool): If True, uses GATConv; otherwise, uses GCNConv.\n",
    "        \"\"\"\n",
    "        super(GraphConditioningModule, self).__init__()\n",
    "\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        if use_attention:\n",
    "            self.gnn1 = GATConv(1, hidden_dim)\n",
    "            self.gnn2 = GATConv(hidden_dim, hidden_dim)\n",
    "        else:\n",
    "            self.gnn1 = GCNConv(1, hidden_dim)\n",
    "            self.gnn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "    # end init\n",
    "\n",
    "    def forward(self, batch_graph, node_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_graph (Batch): Batched graph object from PyG\n",
    "            node_indices (torch.Tensor): Shape (batch_size,), selected node per sample\n",
    "        \n",
    "        Returns:\n",
    "            condition_vectors (torch.Tensor): Shape (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        x = torch.ones((batch_graph.num_nodes, 1), device=batch_graph.edge_index.device)  # Dummy features\n",
    "\n",
    "        x = F.relu(self.gnn1(x, batch_graph.edge_index))\n",
    "        x = F.relu(self.gnn2(x, batch_graph.edge_index))\n",
    "        \n",
    "        node_embeddings = x[node_indices]  # Shape: (batch_size, hidden_dim)\n",
    "        condition_vectors = self.fc(node_embeddings)  # Shape: (batch_size, out_dim)\n",
    "\n",
    "        return condition_vectors\n",
    "    # end forward\n",
    "# end class GraphConditioningModule\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        BiLSTM encoder for sequential input data.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension per timestep\n",
    "            hidden_dim (int): Hidden state dimension\n",
    "        \"\"\"\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)  # Project bidirectional output\n",
    "    # end init\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            hidden_state (torch.Tensor): Shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_n = torch.cat((h_n[0], h_n[1]), dim=-1)  # Concatenate bidirectional outputs\n",
    "        return self.fc(h_n)  # Shape: (batch_size, hidden_dim)\n",
    "    # end forward\n",
    "# end class BiLSTMEncoder\n",
    "\n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        BiLSTM decoder that reconstructs sequences from latent representations.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Hidden dimension of LSTM\n",
    "            output_dim (int): Output feature dimension per timestep\n",
    "        \"\"\"\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    # end init\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent variable (batch_size, hidden_dim)\n",
    "            seq_len (int): Target sequence length\n",
    "        \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Shape (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        z = z.unsqueeze(1).repeat(1, seq_len, 1)  # Expand latent state across sequence\n",
    "        output, _ = self.lstm(z)\n",
    "        return self.fc(output)  # Shape: (batch_size, seq_len, output_dim)\n",
    "    # end forward\n",
    "# end class BiLSTMDecoder\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, transformer_dim, **config):\n",
    "        \"\"\"\n",
    "        CVAE model integrating BiLSTM encoder-decoder and GNN-based conditioning.\n",
    "\n",
    "        Args:\n",
    "            transformer_dim (int): Input and output feature dimension per timestep\n",
    "            hidden_dim_LSTM (int): Hidden dimension for BiLSTM\n",
    "            hidden_dim_GNN (int): Hidden dimension for GNN\n",
    "            latent_dim (int): Dimension of the VAE latent space\n",
    "            condition_dim (int): Dimension of the conditioning vector\n",
    "            use_attention (bool): If True, uses GATConv; otherwise, uses GCNConv.\n",
    "        \"\"\"\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        hidden_dim_LSTM = 256\n",
    "        hidden_dim_GNN = 256\n",
    "        latent_dim = 256\n",
    "        condition_dim = 128\n",
    "        use_attention=False\n",
    "        if 'hidden_dim_LSTM' in config.keys():\n",
    "            hidden_dim_LSTM = config['hidden_dim_LSTM']\n",
    "        if 'hidden_dim_GNN' in config.keys():\n",
    "            hidden_dim_GNN = config['hidden_dim_GNN']\n",
    "        if 'latent_dim' in config.keys():\n",
    "            latent_dim = config['latent_dim']\n",
    "        if 'condition_dim' in config.keys():\n",
    "            condition_dim = config['condition_dim']\n",
    "        if 'use_attention' in config.keys():\n",
    "            use_attention = config['use_attention']\n",
    "\n",
    "        self.lstm_encoder = BiLSTMEncoder(transformer_dim, hidden_dim_LSTM)\n",
    "        self.lstm_decoder = BiLSTMDecoder(hidden_dim_LSTM, transformer_dim)\n",
    "\n",
    "        self.graph_conditioning = GraphConditioningModule(hidden_dim_GNN, condition_dim, use_attention=use_attention)\n",
    "\n",
    "        # Latent space transformations\n",
    "        self.fc_mu = nn.Linear(hidden_dim_LSTM + condition_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim_LSTM + condition_dim, latent_dim)\n",
    "        self.fc_z = nn.Linear(latent_dim + condition_dim, hidden_dim_LSTM)\n",
    "    # end init\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: z = mu + std * epsilon\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    # end reparametrize\n",
    "\n",
    "    def build_batch_graphs(self, markov_matrices):\n",
    "        \"\"\"\n",
    "        Converts a batch of Markov transition matrices into a single batched PyTorch Geometric graph.\n",
    "\n",
    "        Args:\n",
    "            markov_matrices (torch.Tensor): (batch_size, num_nodes, num_nodes) tensor\n",
    "\n",
    "        Returns:\n",
    "            batch_graph (Batch): Batched PyG graph containing all transition matrices\n",
    "            node_indices (torch.Tensor): (batch_size,) tensor containing a node index per sample\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, _ = markov_matrices.shape\n",
    "        graphs = []\n",
    "        node_indices = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Extract nonzero entries (source, target) where transition probability > 0\n",
    "            source_nodes, target_nodes = torch.nonzero(markov_matrices[b], as_tuple=True)\n",
    "            edge_probs = markov_matrices[b][source_nodes, target_nodes]  # Extract transition probabilities\n",
    "\n",
    "            # Create edge_index\n",
    "            edge_index = torch.stack([source_nodes, target_nodes], dim=0)  # Shape (2, num_edges)\n",
    "            \n",
    "            # Create graph data object\n",
    "            graph = Data(edge_index=edge_index, edge_attr=edge_probs, num_nodes=num_nodes)\n",
    "            graphs.append(graph)\n",
    "\n",
    "            # Select a random node to condition on (or use a rule)\n",
    "            node_indices.append(torch.randint(0, num_nodes, (1,)))\n",
    "\n",
    "        # Batch all graphs into a single PyG Batch object\n",
    "        batch_graph = Batch.from_data_list(graphs)\n",
    "        node_indices = torch.cat(node_indices)  # Shape (batch_size,)\n",
    "\n",
    "        return batch_graph, node_indices\n",
    "    # end build_batch_graphs\n",
    "\n",
    "    def forward(self, x, transitions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "            transitions: markov matrix\n",
    "        \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Reconstructed sequence\n",
    "            mu (torch.Tensor): Mean of latent distribution\n",
    "            logvar (torch.Tensor): Log variance of latent distribution\n",
    "        \"\"\"\n",
    "        h = self.lstm_encoder(x)  # Shape: (batch_size, hidden_dim)\n",
    "        batch_graph, node_indices = self.build_batch_graphs( transitions )\n",
    "        condition = self.graph_conditioning(batch_graph, node_indices)  # Shape: (batch_size, condition_dim)\n",
    "\n",
    "        h_cond = torch.cat([h, condition], dim=-1)  # Shape: (batch_size, hidden_dim_LSTM + condition_dim)\n",
    "\n",
    "        mu = self.fc_mu(h_cond)\n",
    "        logvar = self.fc_logvar(h_cond)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        z_cond = torch.cat([z, condition], dim=-1)\n",
    "        z_hidden = self.fc_z(z_cond)  # Shape: (batch_size, hidden_dim_LSTM)\n",
    "\n",
    "        recon_x = self.lstm_decoder(z_hidden, x.shape[1])  # Reconstruct sequence\n",
    "\n",
    "        return recon_x, mu, logvar\n",
    "    # end forward\n",
    "# end CVAE\n",
    "\n",
    "class TransGraphVAE(nn.Module):\n",
    "    def __init__(self, t_encoder, t_decoder, **config):\n",
    "        \"\"\"\n",
    "        TransGraphVAE model that involves a GNN-conditioned BiLSTM VAE between a pretrained\n",
    "        frozen transformer encoder-decoder.\n",
    "\n",
    "        Args:\n",
    "            t_encoder: frozen encoder of the pretrained transformer\n",
    "            t_decoder: frozen encoder of the pretrained transformer\n",
    "            **config: arguments for the CVAE module\n",
    "        \"\"\"\n",
    "        super(TransGraphVAE, self).__init__()\n",
    "        self.t_encoder = t_encoder\n",
    "        self.t_decoder = t_decoder\n",
    "        self.cvae = CVAE(t_encoder.dim, **config)\n",
    "    # end init\n",
    "\n",
    "    def compute_loss(self, recon_x, x, mu, logvar):\n",
    "        \"\"\"\n",
    "        Compute VAE loss (Reconstruction Loss + KL Divergence).\n",
    "        \n",
    "        Args:\n",
    "            recon_x (torch.Tensor): Reconstructed sequences (batch_size, seq_len, transformer_dim)\n",
    "            x (torch.Tensor): Ground truth sequences (batch_size, seq_len, transformer_dim)\n",
    "            mu (torch.Tensor): Mean of latent distribution (batch_size, latent_dim)\n",
    "            logvar (torch.Tensor): Log variance of latent distribution (batch_size, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            loss (torch.Tensor): Combined loss\n",
    "        \"\"\"\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "    # end compute_loss\n",
    "\n",
    "    def forward(self, x, transitions):\n",
    "        x = self.t_encoder(x).last_hidden_state\n",
    "        recon_x, mu, logvar  = self.cvae(x, transitions)\n",
    "        total_loss, _, _ = compute_loss(recon_x, x, mu, logvar)\n",
    "        y_recon = self.t_decoder( recon_x ) # output from reconstruction\n",
    "        y = self.t_decoder( x ) # normal output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_graph, node_indices = build_batch_graphs( b['transitions'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 204], edge_attr=[204], num_nodes=11136, batch=[11136], ptr=[33])\n",
      "tensor([ 10, 139,  48,  25,  97, 252, 217,  64, 211, 134,  96, 249, 150, 172,\n",
      "         90, 292, 185,  87, 207, 221,  11,  50,  88, 181, 180,  47,  75,  34,\n",
      "        287, 244,  19, 221])\n"
     ]
    }
   ],
   "source": [
    "print(batch_graph)\n",
    "print(node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,  58],\n",
      "        [ 58, 116],\n",
      "        [ 59, 116],\n",
      "        [116, 262],\n",
      "        [203,   0],\n",
      "        [262,  58],\n",
      "        [262,  59],\n",
      "        [262, 203],\n",
      "        [262, 262]])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'][0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 9], edge_attr=[9], num_nodes=348)\n"
     ]
    }
   ],
   "source": [
    "ex0 = batch_graph.get_example(0)\n",
    "print(ex0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conditioning = GraphConditioningModule(\n",
    "    hidden_dim=256, out_dim=128, use_attention=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = graph_conditioning(batch_graph, node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vae_loss(recon_x, x, mu, logvar):\n",
    "#     \"\"\"Computes VAE loss (Reconstruction + KL Divergence).\"\"\"\n",
    "#     recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")  # Change to CE for text\n",
    "#     kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL Divergence\n",
    "#     return recon_loss + kl_div\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # === Generate a new batch of Markov transition matrices === #\n",
    "#     markov_matrices = torch.rand(batch_size, num_nodes, num_nodes)  # Example: Random transition matrices\n",
    "#     markov_matrices = markov_matrices / markov_matrices.sum(dim=-1, keepdim=True)  # Normalize rows\n",
    "\n",
    "#     # === Convert batch of matrices into a batched PyG graph === #\n",
    "#     batch_graph, node_indices = build_batch_graphs(markov_matrices)\n",
    "\n",
    "#     # === Generate Random Input Data (Replace with real input) === #\n",
    "#     input_ids = torch.randint(0, 1000, (batch_size, seq_length))  # Example tokenized input\n",
    "#     attention_mask = torch.ones_like(input_ids)  # Dummy attention mask\n",
    "\n",
    "#     # === Forward Pass (Now batch-processed) === #\n",
    "#     recon_x, mu, logvar = cvae(input_ids, attention_mask, batch_graph, node_indices)\n",
    "    \n",
    "#     total_loss = vae_loss(recon_x, input_ids, mu, logvar)\n",
    "#     total_loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch}, Loss: {total_loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
