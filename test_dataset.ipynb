{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import SeparatedMelHarmMarkovDataset\n",
    "import os\n",
    "import numpy as np\n",
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, GCTRootPCTokenizer, \\\n",
    "    GCTSymbolTokenizer, GCTRootTypeTokenizer, MelodyPitchTokenizer, \\\n",
    "    MergedMelHarmTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/ssd2/maximos/data/hooktheory_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordSymbolTokenizer = ChordSymbolTokenizer.from_pretrained('saved_tokenizers/ChordSymbolTokenizer')\n",
    "rootTypeTokenizer = RootTypeTokenizer.from_pretrained('saved_tokenizers/RootTypeTokenizer')\n",
    "pitchClassTokenizer = PitchClassTokenizer.from_pretrained('saved_tokenizers/PitchClassTokenizer')\n",
    "rootPCTokenizer = RootPCTokenizer.from_pretrained('saved_tokenizers/RootPCTokenizer')\n",
    "melodyPitchTokenizer = MelodyPitchTokenizer.from_pretrained('saved_tokenizers/MelodyPitchTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_chordSymbolTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, chordSymbolTokenizer)\n",
    "m_rootTypeTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootTypeTokenizer)\n",
    "m_pitchClassTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, pitchClassTokenizer)\n",
    "m_rootPCTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootPCTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChordSymbolTokenizer\n"
     ]
    }
   ],
   "source": [
    "print(m_chordSymbolTokenizer.harmony_tokenizer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = m_chordSymbolTokenizer\n",
    "\n",
    "dataset = SeparatedMelHarmMarkovDataset(root_dir, tokenizer, max_length=512, num_bars=64)\n",
    "# Data collator for BART\n",
    "def create_data_collator(tokenizer, model):\n",
    "    return DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = BartConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    forced_eos_token_id=tokenizer.eos_token_id,\n",
    "    max_position_embeddings=512,\n",
    "    encoder_layers=8,\n",
    "    encoder_attention_heads=8,\n",
    "    encoder_ffn_dim=512,\n",
    "    decoder_layers=8,\n",
    "    decoder_attention_heads=8,\n",
    "    decoder_ffn_dim=512,\n",
    "    d_model=512,\n",
    "    encoder_layerdrop=0.3,\n",
    "    decoder_layerdrop=0.3,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "model = BartForConditionalGeneration(bart_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = create_data_collator(tokenizer, model=model)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/.local/lib/python3.11/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "/home/maximos/.local/lib/python3.11/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'][5].sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 348, 348])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "def build_batch_graphs(markov_matrices):\n",
    "    \"\"\"\n",
    "    Converts a batch of Markov transition matrices into a single batched PyTorch Geometric graph.\n",
    "\n",
    "    Args:\n",
    "        markov_matrices (torch.Tensor): (batch_size, num_nodes, num_nodes) tensor\n",
    "\n",
    "    Returns:\n",
    "        batch_graph (Batch): Batched PyG graph containing all transition matrices\n",
    "        node_indices (torch.Tensor): (batch_size,) tensor containing a node index per sample\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, _ = markov_matrices.shape\n",
    "    graphs = []\n",
    "    node_indices = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        # Extract nonzero entries (source, target) where transition probability > 0\n",
    "        source_nodes, target_nodes = torch.nonzero(markov_matrices[b], as_tuple=True)\n",
    "        edge_probs = markov_matrices[b][source_nodes, target_nodes]  # Extract transition probabilities\n",
    "\n",
    "        # Create edge_index\n",
    "        edge_index = torch.stack([source_nodes, target_nodes], dim=0)  # Shape (2, num_edges)\n",
    "        \n",
    "        # Create graph data object\n",
    "        graph = Data(edge_index=edge_index, edge_attr=edge_probs, num_nodes=num_nodes)\n",
    "        graphs.append(graph)\n",
    "\n",
    "        # Select a random node to condition on (or use a rule)\n",
    "        node_indices.append(torch.randint(0, num_nodes, (1,)))\n",
    "\n",
    "    # Batch all graphs into a single PyG Batch object\n",
    "    batch_graph = Batch.from_data_list(graphs)\n",
    "    node_indices = torch.cat(node_indices)  # Shape (batch_size,)\n",
    "\n",
    "    return batch_graph, node_indices\n",
    "# end build_batch_graphs\n",
    "\n",
    "class GraphConditioningModule(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim, use_attention=False):\n",
    "        \"\"\"\n",
    "        Graph-based conditioning module for extracting node embeddings as condition vectors.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Hidden dimension of GNN layers\n",
    "            out_dim (int): Dimension of the conditioning vector\n",
    "            use_attention (bool): If True, uses GATConv; otherwise, uses GCNConv.\n",
    "        \"\"\"\n",
    "        super(GraphConditioningModule, self).__init__()\n",
    "\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        if use_attention:\n",
    "            self.gnn1 = GATConv(1, hidden_dim)\n",
    "            self.gnn2 = GATConv(hidden_dim, hidden_dim)\n",
    "        else:\n",
    "            self.gnn1 = GCNConv(1, hidden_dim)\n",
    "            self.gnn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "    # end init\n",
    "\n",
    "    def forward(self, batch_graph, node_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_graph (Batch): Batched graph object from PyG\n",
    "            node_indices (torch.Tensor): Shape (batch_size,), selected node per sample\n",
    "        \n",
    "        Returns:\n",
    "            condition_vectors (torch.Tensor): Shape (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        x = torch.ones((batch_graph.num_nodes, 1), device=batch_graph.edge_index.device)  # Dummy features\n",
    "\n",
    "        x = F.relu(self.gnn1(x, batch_graph.edge_index))\n",
    "        x = F.relu(self.gnn2(x, batch_graph.edge_index))\n",
    "        \n",
    "        node_embeddings = x[node_indices]  # Shape: (batch_size, hidden_dim)\n",
    "        condition_vectors = self.fc(node_embeddings)  # Shape: (batch_size, out_dim)\n",
    "\n",
    "        return condition_vectors\n",
    "    # end forward\n",
    "# end class GraphConditioningModule\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        BiLSTM encoder for sequential input data.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension per timestep\n",
    "            hidden_dim (int): Hidden state dimension\n",
    "        \"\"\"\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)  # Project bidirectional output\n",
    "    # end init\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            hidden_state (torch.Tensor): Shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_n = torch.cat((h_n[0], h_n[1]), dim=-1)  # Concatenate bidirectional outputs\n",
    "        return self.fc(h_n)  # Shape: (batch_size, hidden_dim)\n",
    "    # end forward\n",
    "# end class BiLSTMEncoder\n",
    "\n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        BiLSTM decoder that reconstructs sequences from latent representations.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Hidden dimension of LSTM\n",
    "            output_dim (int): Output feature dimension per timestep\n",
    "        \"\"\"\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    # end init\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent variable (batch_size, hidden_dim)\n",
    "            seq_len (int): Target sequence length\n",
    "        \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Shape (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        z = z.unsqueeze(1).repeat(1, seq_len, 1)  # Expand latent state across sequence\n",
    "        output, _ = self.lstm(z)\n",
    "        return self.fc(output)  # Shape: (batch_size, seq_len, output_dim)\n",
    "    # end forward\n",
    "# end class BiLSTMDecoder\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, condition_dim, output_dim, use_attention=False):\n",
    "        \"\"\"\n",
    "        CVAE model integrating BiLSTM encoder-decoder and GNN-based conditioning.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension per timestep\n",
    "            hidden_dim (int): Hidden dimension for BiLSTM\n",
    "            latent_dim (int): Dimension of the latent space\n",
    "            condition_dim (int): Dimension of the conditioning vector\n",
    "            output_dim (int): Output feature dimension per timestep\n",
    "            use_attention (bool): If True, uses GATConv; otherwise, uses GCNConv.\n",
    "        \"\"\"\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.lstm_encoder = BiLSTMEncoder(input_dim, hidden_dim)\n",
    "        self.lstm_decoder = BiLSTMDecoder(hidden_dim, output_dim)\n",
    "\n",
    "        self.graph_conditioning = GraphConditioningModule(hidden_dim, condition_dim, use_attention=use_attention)\n",
    "\n",
    "        # Latent space transformations\n",
    "        self.fc_mu = nn.Linear(hidden_dim + condition_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim + condition_dim, latent_dim)\n",
    "        self.fc_z = nn.Linear(latent_dim + condition_dim, hidden_dim)\n",
    "    # end init\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: z = mu + std * epsilon\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    # end reparametrize\n",
    "\n",
    "    def forward(self, x, transitions, seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "            transitions: markov matrix\n",
    "            seq_len (int): Target sequence length\n",
    "        \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Reconstructed sequence\n",
    "            mu (torch.Tensor): Mean of latent distribution\n",
    "            logvar (torch.Tensor): Log variance of latent distribution\n",
    "        \"\"\"\n",
    "        h = self.lstm_encoder(x)  # Shape: (batch_size, hidden_dim)\n",
    "        batch_graph, node_indices = build_batch_graphs( transitions )\n",
    "        condition = self.graph_conditioning(batch_graph, node_indices)  # Shape: (batch_size, condition_dim)\n",
    "\n",
    "        h_cond = torch.cat([h, condition], dim=-1)  # Shape: (batch_size, hidden_dim + condition_dim)\n",
    "\n",
    "        mu = self.fc_mu(h_cond)\n",
    "        logvar = self.fc_logvar(h_cond)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        z_cond = torch.cat([z, condition], dim=-1)\n",
    "        z_hidden = self.fc_z(z_cond)  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        recon_x = self.lstm_decoder(z_hidden, seq_len)  # Reconstruct sequence\n",
    "\n",
    "        return recon_x, mu, logvar\n",
    "    # end forward\n",
    "# end CVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_graph, node_indices = build_batch_graphs( b['transitions'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 268], edge_attr=[268], num_nodes=11136, batch=[11136], ptr=[33])\n",
      "tensor([ 53, 130,  77, 188,  68, 227, 205, 250, 200, 184, 149, 334,  23, 257,\n",
      "         18, 199, 324, 118, 120, 345, 144, 342,  26, 215, 186, 125,  29, 128,\n",
      "        212,  72, 346,  79])\n"
     ]
    }
   ],
   "source": [
    "print(batch_graph)\n",
    "print(node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0],\n",
      "        [  0,  66],\n",
      "        [  0, 203],\n",
      "        [ 58,  58],\n",
      "        [ 58, 203],\n",
      "        [ 66,   0],\n",
      "        [ 66, 262],\n",
      "        [203, 203],\n",
      "        [203, 209],\n",
      "        [209,   0],\n",
      "        [209, 209],\n",
      "        [262, 262],\n",
      "        [262, 269],\n",
      "        [269,  58],\n",
      "        [269, 269]])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'][0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 15], edge_attr=[15], num_nodes=348)\n"
     ]
    }
   ],
   "source": [
    "ex0 = batch_graph.get_example(0)\n",
    "print(ex0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conditioning = GraphConditioningModule(\n",
    "    hidden_dim=256, out_dim=128, use_attention=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = graph_conditioning(batch_graph, node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vae_loss(recon_x, x, mu, logvar):\n",
    "#     \"\"\"Computes VAE loss (Reconstruction + KL Divergence).\"\"\"\n",
    "#     recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")  # Change to CE for text\n",
    "#     kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL Divergence\n",
    "#     return recon_loss + kl_div\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # === Generate a new batch of Markov transition matrices === #\n",
    "#     markov_matrices = torch.rand(batch_size, num_nodes, num_nodes)  # Example: Random transition matrices\n",
    "#     markov_matrices = markov_matrices / markov_matrices.sum(dim=-1, keepdim=True)  # Normalize rows\n",
    "\n",
    "#     # === Convert batch of matrices into a batched PyG graph === #\n",
    "#     batch_graph, node_indices = build_batch_graphs(markov_matrices)\n",
    "\n",
    "#     # === Generate Random Input Data (Replace with real input) === #\n",
    "#     input_ids = torch.randint(0, 1000, (batch_size, seq_length))  # Example tokenized input\n",
    "#     attention_mask = torch.ones_like(input_ids)  # Dummy attention mask\n",
    "\n",
    "#     # === Forward Pass (Now batch-processed) === #\n",
    "#     recon_x, mu, logvar = cvae(input_ids, attention_mask, batch_graph, node_indices)\n",
    "    \n",
    "#     total_loss = vae_loss(recon_x, input_ids, mu, logvar)\n",
    "#     total_loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch}, Loss: {total_loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
