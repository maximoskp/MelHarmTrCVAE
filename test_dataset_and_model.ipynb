{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import SeparatedMelHarmMarkovDataset\n",
    "import os\n",
    "import numpy as np\n",
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, GCTRootPCTokenizer, \\\n",
    "    GCTSymbolTokenizer, GCTRootTypeTokenizer, MelodyPitchTokenizer, \\\n",
    "    MergedMelHarmTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/ssd2/maximos/data/hooktheory_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordSymbolTokenizer = ChordSymbolTokenizer.from_pretrained('saved_tokenizers/ChordSymbolTokenizer')\n",
    "rootTypeTokenizer = RootTypeTokenizer.from_pretrained('saved_tokenizers/RootTypeTokenizer')\n",
    "pitchClassTokenizer = PitchClassTokenizer.from_pretrained('saved_tokenizers/PitchClassTokenizer')\n",
    "rootPCTokenizer = RootPCTokenizer.from_pretrained('saved_tokenizers/RootPCTokenizer')\n",
    "melodyPitchTokenizer = MelodyPitchTokenizer.from_pretrained('saved_tokenizers/MelodyPitchTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_chordSymbolTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, chordSymbolTokenizer)\n",
    "m_rootTypeTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootTypeTokenizer)\n",
    "m_pitchClassTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, pitchClassTokenizer)\n",
    "m_rootPCTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootPCTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChordSymbolTokenizer\n"
     ]
    }
   ],
   "source": [
    "print(m_chordSymbolTokenizer.harmony_tokenizer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = m_chordSymbolTokenizer\n",
    "tokenizer_name = 'ChordSymbolTokenizer'\n",
    "\n",
    "dataset = SeparatedMelHarmMarkovDataset(root_dir, tokenizer, max_length=512, num_bars=64)\n",
    "# Data collator for BART\n",
    "def create_data_collator(tokenizer, model):\n",
    "    return DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/.local/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "bart_config = BartConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    forced_eos_token_id=tokenizer.eos_token_id,\n",
    "    max_position_embeddings=512,\n",
    "    encoder_layers=8,\n",
    "    encoder_attention_heads=8,\n",
    "    encoder_ffn_dim=512,\n",
    "    decoder_layers=8,\n",
    "    decoder_attention_heads=8,\n",
    "    decoder_ffn_dim=512,\n",
    "    d_model=512,\n",
    "    encoder_layerdrop=0.3,\n",
    "    decoder_layerdrop=0.3,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "bart = BartForConditionalGeneration(bart_config)\n",
    "\n",
    "bart_path = 'saved_models/bart/' + tokenizer_name + '/' + tokenizer_name + '.pt'\n",
    "if device == 'cpu':\n",
    "    checkpoint = torch.load(bart_path, map_location=\"cpu\", weights_only=True)\n",
    "else:\n",
    "    checkpoint = torch.load(bart_path, weights_only=True)\n",
    "bart.load_state_dict(checkpoint)\n",
    "\n",
    "bart.to(device)\n",
    "bart.eval()\n",
    "\n",
    "bart_encoder, bart_decoder = bart.get_encoder(), bart.get_decoder()\n",
    "bart_encoder.to(device)\n",
    "bart_decoder.to(device)\n",
    "\n",
    "# Freeze BART parameters\n",
    "for param in bart_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in bart_encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = create_data_collator(tokenizer, model=bart)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/.local/lib/python3.11/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/.local/lib/python3.11/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'][5].sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 348, 348])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "def build_batch_graphs(markov_matrices):\n",
    "    \"\"\"\n",
    "    Converts a batch of Markov transition matrices into a single batched PyTorch Geometric graph.\n",
    "\n",
    "    Args:\n",
    "        markov_matrices (torch.Tensor): (batch_size, num_nodes, num_nodes) tensor\n",
    "\n",
    "    Returns:\n",
    "        batch_graph (Batch): Batched PyG graph containing all transition matrices\n",
    "        node_indices (torch.Tensor): (batch_size,) tensor containing a node index per sample\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, _ = markov_matrices.shape\n",
    "    graphs = []\n",
    "    node_indices = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        # Extract nonzero entries (source, target) where transition probability > 0\n",
    "        source_nodes, target_nodes = torch.nonzero(markov_matrices[b], as_tuple=True)\n",
    "        edge_probs = markov_matrices[b][source_nodes, target_nodes]  # Extract transition probabilities\n",
    "\n",
    "        # Create edge_index\n",
    "        edge_index = torch.stack([source_nodes, target_nodes], dim=0)  # Shape (2, num_edges)\n",
    "        \n",
    "        # Create graph data object\n",
    "        graph = Data(edge_index=edge_index, edge_attr=edge_probs, num_nodes=num_nodes)\n",
    "        graphs.append(graph)\n",
    "\n",
    "        # Select a random node to condition on (or use a rule)\n",
    "        node_indices.append(torch.randint(0, num_nodes, (1,)))\n",
    "\n",
    "    # Batch all graphs into a single PyG Batch object\n",
    "    batch_graph = Batch.from_data_list(graphs)\n",
    "    node_indices = torch.cat(node_indices)  # Shape (batch_size,)\n",
    "\n",
    "    return batch_graph, node_indices\n",
    "# end build_batch_graphs\n",
    "\n",
    "def compute_loss(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Compute VAE loss (Reconstruction Loss + KL Divergence).\n",
    "    \n",
    "    Args:\n",
    "        recon_x (torch.Tensor): Reconstructed sequences (batch_size, seq_len, transformer_dim)\n",
    "        x (torch.Tensor): Ground truth sequences (batch_size, seq_len, transformer_dim)\n",
    "        mu (torch.Tensor): Mean of latent distribution (batch_size, latent_dim)\n",
    "        logvar (torch.Tensor): Log variance of latent distribution (batch_size, latent_dim)\n",
    "    \n",
    "    Returns:\n",
    "        loss (torch.Tensor): Combined loss\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "# end compute_loss\n",
    "\n",
    "class GraphConditioningModule(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim, use_attention=False):\n",
    "        \"\"\"\n",
    "        Graph-based conditioning module for extracting node embeddings as condition vectors.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Hidden dimension of GNN layers\n",
    "            out_dim (int): Dimension of the conditioning vector\n",
    "            use_attention (bool): If True, uses GATConv; otherwise, uses GCNConv.\n",
    "        \"\"\"\n",
    "        super(GraphConditioningModule, self).__init__()\n",
    "\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        if use_attention:\n",
    "            self.gnn1 = GATConv(1, hidden_dim)\n",
    "            self.gnn2 = GATConv(hidden_dim, hidden_dim)\n",
    "        else:\n",
    "            self.gnn1 = GCNConv(1, hidden_dim)\n",
    "            self.gnn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "    # end init\n",
    "\n",
    "    def forward(self, batch_graph, node_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_graph (Batch): Batched graph object from PyG\n",
    "            node_indices (torch.Tensor): Shape (batch_size,), selected node per sample\n",
    "        \n",
    "        Returns:\n",
    "            condition_vectors (torch.Tensor): Shape (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        x = torch.ones((batch_graph.num_nodes, 1), device=batch_graph.edge_index.device)  # Dummy features\n",
    "\n",
    "        x = F.relu(self.gnn1(x, batch_graph.edge_index))\n",
    "        x = F.relu(self.gnn2(x, batch_graph.edge_index))\n",
    "        \n",
    "        node_embeddings = x[node_indices]  # Shape: (batch_size, hidden_dim)\n",
    "        condition_vectors = self.fc(node_embeddings)  # Shape: (batch_size, out_dim)\n",
    "\n",
    "        return condition_vectors\n",
    "    # end forward\n",
    "# end class GraphConditioningModule\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        BiLSTM encoder for sequential input data.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension per timestep\n",
    "            hidden_dim (int): Hidden state dimension\n",
    "        \"\"\"\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)  # Project bidirectional output\n",
    "    # end init\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            hidden_state (torch.Tensor): Shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_n = torch.cat((h_n[0], h_n[1]), dim=-1)  # Concatenate bidirectional outputs\n",
    "        return self.fc(h_n)  # Shape: (batch_size, hidden_dim)\n",
    "    # end forward\n",
    "# end class BiLSTMEncoder\n",
    "\n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        BiLSTM decoder that reconstructs sequences from latent representations.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Hidden dimension of LSTM\n",
    "            output_dim (int): Output feature dimension per timestep\n",
    "        \"\"\"\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    # end init\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent variable (batch_size, hidden_dim)\n",
    "            seq_len (int): Target sequence length\n",
    "        \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Shape (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        z = z.unsqueeze(1).repeat(1, seq_len, 1)  # Expand latent state across sequence\n",
    "        output, _ = self.lstm(z)\n",
    "        return self.fc(output)  # Shape: (batch_size, seq_len, output_dim)\n",
    "    # end forward\n",
    "# end class BiLSTMDecoder\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, transformer_dim, **config):\n",
    "        \"\"\"\n",
    "        CVAE model integrating BiLSTM encoder-decoder and GNN-based conditioning.\n",
    "\n",
    "        Args:\n",
    "            transformer_dim (int): Input and output feature dimension per timestep\n",
    "            hidden_dim_LSTM (int): Hidden dimension for BiLSTM\n",
    "            hidden_dim_GNN (int): Hidden dimension for GNN\n",
    "            latent_dim (int): Dimension of the VAE latent space\n",
    "            condition_dim (int): Dimension of the conditioning vector\n",
    "            use_attention (bool): If True, uses GATConv; otherwise, uses GCNConv.\n",
    "        \"\"\"\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        hidden_dim_LSTM = 256\n",
    "        hidden_dim_GNN = 256\n",
    "        latent_dim = 256\n",
    "        condition_dim = 128\n",
    "        use_attention=False\n",
    "        if 'hidden_dim_LSTM' in config.keys():\n",
    "            hidden_dim_LSTM = config['hidden_dim_LSTM']\n",
    "        if 'hidden_dim_GNN' in config.keys():\n",
    "            hidden_dim_GNN = config['hidden_dim_GNN']\n",
    "        if 'latent_dim' in config.keys():\n",
    "            latent_dim = config['latent_dim']\n",
    "        if 'condition_dim' in config.keys():\n",
    "            condition_dim = config['condition_dim']\n",
    "        if 'use_attention' in config.keys():\n",
    "            use_attention = config['use_attention']\n",
    "\n",
    "        self.lstm_encoder = BiLSTMEncoder(transformer_dim, hidden_dim_LSTM)\n",
    "        self.lstm_decoder = BiLSTMDecoder(hidden_dim_LSTM, transformer_dim)\n",
    "\n",
    "        self.graph_conditioning = GraphConditioningModule(hidden_dim_GNN, condition_dim, use_attention=use_attention)\n",
    "\n",
    "        # Latent space transformations\n",
    "        self.fc_mu = nn.Linear(hidden_dim_LSTM + condition_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim_LSTM + condition_dim, latent_dim)\n",
    "        self.fc_z = nn.Linear(latent_dim + condition_dim, hidden_dim_LSTM)\n",
    "    # end init\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: z = mu + std * epsilon\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    # end reparametrize\n",
    "\n",
    "    def build_batch_graphs(self, markov_matrices):\n",
    "        \"\"\"\n",
    "        Converts a batch of Markov transition matrices into a single batched PyTorch Geometric graph.\n",
    "\n",
    "        Args:\n",
    "            markov_matrices (torch.Tensor): (batch_size, num_nodes, num_nodes) tensor\n",
    "\n",
    "        Returns:\n",
    "            batch_graph (Batch): Batched PyG graph containing all transition matrices\n",
    "            node_indices (torch.Tensor): (batch_size,) tensor containing a node index per sample\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, _ = markov_matrices.shape\n",
    "        graphs = []\n",
    "        node_indices = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Extract nonzero entries (source, target) where transition probability > 0\n",
    "            source_nodes, target_nodes = torch.nonzero(markov_matrices[b], as_tuple=True)\n",
    "            edge_probs = markov_matrices[b][source_nodes, target_nodes]  # Extract transition probabilities\n",
    "\n",
    "            # Create edge_index\n",
    "            edge_index = torch.stack([source_nodes, target_nodes], dim=0)  # Shape (2, num_edges)\n",
    "            \n",
    "            # Create graph data object\n",
    "            graph = Data(edge_index=edge_index, edge_attr=edge_probs, num_nodes=num_nodes)\n",
    "            graphs.append(graph)\n",
    "\n",
    "            # Select a random node to condition on (or use a rule)\n",
    "            node_indices.append(torch.randint(0, num_nodes, (1,)))\n",
    "\n",
    "        # Batch all graphs into a single PyG Batch object\n",
    "        batch_graph = Batch.from_data_list(graphs)\n",
    "        node_indices = torch.cat(node_indices)  # Shape (batch_size,)\n",
    "\n",
    "        return batch_graph, node_indices\n",
    "    # end build_batch_graphs\n",
    "\n",
    "    def forward(self, x, transitions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "            transitions: markov matrix\n",
    "        \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Reconstructed sequence\n",
    "            mu (torch.Tensor): Mean of latent distribution\n",
    "            logvar (torch.Tensor): Log variance of latent distribution\n",
    "        \"\"\"\n",
    "        h = self.lstm_encoder(x)  # Shape: (batch_size, hidden_dim)\n",
    "        batch_graph, node_indices = self.build_batch_graphs( transitions )\n",
    "        condition = self.graph_conditioning(batch_graph, node_indices)  # Shape: (batch_size, condition_dim)\n",
    "\n",
    "        h_cond = torch.cat([h, condition], dim=-1)  # Shape: (batch_size, hidden_dim_LSTM + condition_dim)\n",
    "\n",
    "        mu = self.fc_mu(h_cond)\n",
    "        logvar = self.fc_logvar(h_cond)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        z_cond = torch.cat([z, condition], dim=-1)\n",
    "        z_hidden = self.fc_z(z_cond)  # Shape: (batch_size, hidden_dim_LSTM)\n",
    "\n",
    "        recon_x = self.lstm_decoder(z_hidden, x.shape[1])  # Reconstruct sequence\n",
    "\n",
    "        return recon_x, mu, logvar\n",
    "    # end forward\n",
    "# end CVAE\n",
    "\n",
    "class TransGraphVAE(nn.Module):\n",
    "    def __init__(self, transformer, **config):\n",
    "        \"\"\"\n",
    "        TransGraphVAE model that involves a GNN-conditioned BiLSTM VAE between a pretrained\n",
    "        frozen transformer encoder-decoder.\n",
    "\n",
    "        Args:\n",
    "            t_encoder: frozen encoder of the pretrained transformer\n",
    "            t_decoder: frozen encoder of the pretrained transformer\n",
    "            **config: arguments for the CVAE module\n",
    "        \"\"\"\n",
    "        super(TransGraphVAE, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.t_encoder = transformer.get_encoder()\n",
    "        self.t_decoder = transformer.get_decoder()\n",
    "        self.cvae = CVAE(self.transformer.config.d_model, **config)\n",
    "    # end init\n",
    "\n",
    "    def compute_loss(self, recon_x, x, mu, logvar):\n",
    "        \"\"\"\n",
    "        Compute VAE loss (Reconstruction Loss + KL Divergence).\n",
    "        \n",
    "        Args:\n",
    "            recon_x (torch.Tensor): Reconstructed sequences (batch_size, seq_len, transformer_dim)\n",
    "            x (torch.Tensor): Ground truth sequences (batch_size, seq_len, transformer_dim)\n",
    "            mu (torch.Tensor): Mean of latent distribution (batch_size, latent_dim)\n",
    "            logvar (torch.Tensor): Log variance of latent distribution (batch_size, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            loss (torch.Tensor): Combined loss\n",
    "        \"\"\"\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "    # end compute_loss\n",
    "\n",
    "    def forward(self, x, transitions, generate_max_tokens=-1):\n",
    "        x = self.t_encoder(x).last_hidden_state\n",
    "        recon_x, mu, logvar  = self.cvae(x, transitions)\n",
    "        total_loss, recon_loss, kl_loss = self.compute_loss(recon_x, x, mu, logvar)\n",
    "        y_generated_tokens = None\n",
    "        y_recon_generated_tokens = None\n",
    "        if generate_max_tokens > 0:\n",
    "            # TODO: implement autoregressive process with temperature\n",
    "            y_recon = self.t_decoder( recon_x ) # output from reconstruction\n",
    "            y = self.t_decoder( x ) # normal output\n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'x': x,\n",
    "            'recon_x': recon_x,\n",
    "            'y_tokens': y_generated_tokens,\n",
    "            'recon_y_tokens': y_recon_generated_tokens\n",
    "        }\n",
    "    # end forward\n",
    "# end class TransGraphVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_graph, node_indices = build_batch_graphs( b['transitions'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 287], edge_attr=[287], num_nodes=11136, batch=[11136], ptr=[33])\n",
      "tensor([325, 124, 227, 118, 201, 169, 128, 293, 310, 112, 307,  33, 272, 133,\n",
      "        222,  33, 191,  71, 110, 118,  16, 194, 123,  31, 244, 277,  87, 203,\n",
      "        124,  48,  92, 332])\n"
     ]
    }
   ],
   "source": [
    "print(batch_graph)\n",
    "print(node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 59, 262],\n",
      "        [ 66, 152],\n",
      "        [122, 122],\n",
      "        [152, 122],\n",
      "        [203,  66],\n",
      "        [262, 203]])\n"
     ]
    }
   ],
   "source": [
    "print(b['transitions'][0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 6], edge_attr=[6], num_nodes=348)\n"
     ]
    }
   ],
   "source": [
    "ex0 = batch_graph.get_example(0)\n",
    "print(ex0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conditioning = GraphConditioningModule(\n",
    "    hidden_dim=256, out_dim=128, use_attention=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = graph_conditioning(batch_graph, node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgvae = TransGraphVAE(bart).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = b['input_ids'].to(device)\n",
    "tr = b['transitions'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tgvae( x, tr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.9438, device='cuda:0', grad_fn=<AddBackward0>), 'recon_loss': tensor(0.9417, device='cuda:0', grad_fn=<MseLossBackward0>), 'kl_loss': tensor(0.0021, device='cuda:0', grad_fn=<MulBackward0>), 'x': tensor([[[-0.6707, -0.9266, -0.0838,  ...,  2.5568,  0.6835, -1.4143],\n",
      "         [-0.7106, -0.6665,  1.0344,  ...,  2.7133,  1.6434, -1.2933],\n",
      "         [-0.6275, -1.0995, -0.7869,  ...,  1.8026,  0.5099, -0.4473],\n",
      "         ...,\n",
      "         [ 0.3020, -0.6811, -0.1940,  ...,  0.7174,  0.6095,  0.5433],\n",
      "         [-0.0349, -0.6357, -0.9292,  ...,  1.3062,  0.4276,  0.4997],\n",
      "         [ 0.4289, -0.8289, -0.7258,  ...,  0.9860,  0.7528,  0.2738]],\n",
      "\n",
      "        [[-0.5190, -0.5464,  0.1391,  ...,  2.0915, -0.0181, -1.2419],\n",
      "         [-0.6576, -0.3976,  1.0410,  ...,  2.2047,  1.3650, -1.0917],\n",
      "         [-0.7930, -0.7747, -0.2073,  ...,  1.6193,  0.0416,  0.0135],\n",
      "         ...,\n",
      "         [ 0.2441, -0.4514,  0.3843,  ...,  0.2089, -0.6739,  0.7951],\n",
      "         [ 0.0258, -0.2544, -0.2993,  ...,  0.6862, -0.8254,  0.8025],\n",
      "         [ 0.6206, -0.4567, -0.2576,  ...,  0.3988, -0.5078,  0.3962]],\n",
      "\n",
      "        [[-0.7183, -0.6572, -0.0336,  ...,  2.3683,  0.3567, -1.5708],\n",
      "         [-0.8304, -0.3762,  1.2035,  ...,  2.5696,  1.4265, -1.3484],\n",
      "         [-0.7919, -0.7827, -0.4480,  ...,  1.4310,  0.0093, -0.4684],\n",
      "         ...,\n",
      "         [ 0.4540, -0.6276,  0.2454,  ...,  0.3803,  0.0241,  0.2971],\n",
      "         [ 0.1468, -0.4764, -0.5513,  ...,  0.7019, -0.1479,  0.2853],\n",
      "         [ 0.6948, -0.6607, -0.3518,  ...,  0.5950,  0.1484, -0.0166]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1912, -1.0329, -0.1168,  ...,  2.5585,  0.6715, -1.4239],\n",
      "         [-0.1264, -0.7720,  1.0839,  ...,  2.9735,  1.6775, -1.1205],\n",
      "         [-0.1910, -1.2245, -0.7379,  ...,  2.0744,  0.6166, -0.2283],\n",
      "         ...,\n",
      "         [ 0.5250, -1.1580,  0.0067,  ...,  0.3970,  0.1452,  0.6620],\n",
      "         [ 0.2690, -0.8324, -0.8507,  ...,  1.0636, -0.0540,  0.5758],\n",
      "         [ 0.7200, -1.2580, -0.5920,  ...,  0.6809,  0.3235,  0.3247]],\n",
      "\n",
      "        [[-0.4746, -0.0516,  0.4076,  ...,  0.9140, -0.3688, -1.3893],\n",
      "         [-0.2120, -0.1937,  1.1382,  ...,  1.3248,  1.3152, -1.1475],\n",
      "         [-0.7116, -0.4817, -0.1907,  ...,  1.1124, -0.2215,  0.1771],\n",
      "         ...,\n",
      "         [ 0.1990, -0.0849,  0.0639,  ...,  0.4962,  1.1996, -0.6144],\n",
      "         [ 0.8361, -0.1000, -1.9778,  ...,  1.5821,  0.6936, -1.3329],\n",
      "         [ 1.4975, -0.2199, -0.2115,  ..., -0.6975,  0.9932, -1.3803]],\n",
      "\n",
      "        [[-0.5865,  0.0154,  0.2885,  ...,  1.1869, -0.0705, -1.3672],\n",
      "         [-0.5195, -0.2530,  1.4299,  ...,  1.8543,  1.2352, -1.0470],\n",
      "         [-0.7856, -0.6787, -0.2091,  ...,  0.9499, -0.0755,  0.0588],\n",
      "         ...,\n",
      "         [ 0.3058, -0.7818,  0.0801,  ..., -0.5865, -0.7041,  0.0113],\n",
      "         [ 0.0421, -0.6393, -0.7444,  ..., -0.0874, -1.0432,  0.0340],\n",
      "         [ 0.6706, -0.9748, -0.4522,  ..., -0.0284, -0.6516, -0.3089]]],\n",
      "       device='cuda:0'), 'recon_x': tensor([[[-0.0267, -0.0258, -0.0466,  ..., -0.1008, -0.0295, -0.0043],\n",
      "         [-0.0398, -0.0107, -0.0563,  ..., -0.1194, -0.0476,  0.0109],\n",
      "         [-0.0451, -0.0021, -0.0574,  ..., -0.1260, -0.0567,  0.0188],\n",
      "         ...,\n",
      "         [-0.0526,  0.0089, -0.0498,  ..., -0.1240, -0.0639,  0.0278],\n",
      "         [-0.0526,  0.0089, -0.0498,  ..., -0.1240, -0.0639,  0.0278],\n",
      "         [-0.0526,  0.0089, -0.0498,  ..., -0.1240, -0.0639,  0.0278]],\n",
      "\n",
      "        [[-0.0183, -0.0409, -0.0418,  ..., -0.1090,  0.0419, -0.0505],\n",
      "         [-0.0305, -0.0384, -0.0622,  ..., -0.1298,  0.0693, -0.0572],\n",
      "         [-0.0351, -0.0388, -0.0757,  ..., -0.1353,  0.0863, -0.0597],\n",
      "         ...,\n",
      "         [-0.0364, -0.0393, -0.1000,  ..., -0.1240,  0.1105, -0.0588],\n",
      "         [-0.0364, -0.0393, -0.1000,  ..., -0.1240,  0.1105, -0.0588],\n",
      "         [-0.0364, -0.0394, -0.1000,  ..., -0.1240,  0.1105, -0.0588]],\n",
      "\n",
      "        [[-0.0048, -0.0622, -0.0336,  ..., -0.0266,  0.0136, -0.0573],\n",
      "         [-0.0153, -0.0735, -0.0419,  ..., -0.0205,  0.0133, -0.0704],\n",
      "         [-0.0194, -0.0831, -0.0442,  ..., -0.0197,  0.0131, -0.0787],\n",
      "         ...,\n",
      "         [-0.0155, -0.1043, -0.0434,  ..., -0.0173,  0.0157, -0.0897],\n",
      "         [-0.0155, -0.1043, -0.0434,  ..., -0.0173,  0.0157, -0.0897],\n",
      "         [-0.0155, -0.1043, -0.0434,  ..., -0.0173,  0.0157, -0.0897]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0231, -0.0618, -0.1041,  ..., -0.0098,  0.0036, -0.0864],\n",
      "         [-0.0456, -0.0655, -0.1540,  ...,  0.0031, -0.0012, -0.1141],\n",
      "         [-0.0592, -0.0677, -0.1806,  ...,  0.0080, -0.0048, -0.1282],\n",
      "         ...,\n",
      "         [-0.0853, -0.0714, -0.2137,  ...,  0.0170, -0.0126, -0.1378],\n",
      "         [-0.0853, -0.0714, -0.2137,  ...,  0.0170, -0.0126, -0.1378],\n",
      "         [-0.0853, -0.0714, -0.2137,  ...,  0.0170, -0.0126, -0.1378]],\n",
      "\n",
      "        [[-0.0528,  0.0474, -0.0176,  ..., -0.0306,  0.0307, -0.0969],\n",
      "         [-0.0857,  0.0851, -0.0348,  ..., -0.0205,  0.0386, -0.1310],\n",
      "         [-0.1014,  0.1004, -0.0503,  ..., -0.0151,  0.0414, -0.1511],\n",
      "         ...,\n",
      "         [-0.1177,  0.1160, -0.0828,  ..., -0.0063,  0.0431, -0.1810],\n",
      "         [-0.1177,  0.1160, -0.0828,  ..., -0.0063,  0.0431, -0.1810],\n",
      "         [-0.1177,  0.1160, -0.0828,  ..., -0.0063,  0.0431, -0.1810]],\n",
      "\n",
      "        [[ 0.0043, -0.0701,  0.0147,  ..., -0.0221,  0.0369, -0.0225],\n",
      "         [ 0.0045, -0.0738,  0.0258,  ..., -0.0121,  0.0483, -0.0180],\n",
      "         [ 0.0066, -0.0735,  0.0317,  ..., -0.0079,  0.0529, -0.0163],\n",
      "         ...,\n",
      "         [ 0.0083, -0.0723,  0.0433,  ...,  0.0055,  0.0565, -0.0181],\n",
      "         [ 0.0083, -0.0723,  0.0433,  ...,  0.0055,  0.0565, -0.0181],\n",
      "         [ 0.0083, -0.0723,  0.0433,  ...,  0.0055,  0.0565, -0.0181]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'y_tokens': None, 'recon_y_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['loss'].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(tgvae.cvae.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vae_loss(recon_x, x, mu, logvar):\n",
    "#     \"\"\"Computes VAE loss (Reconstruction + KL Divergence).\"\"\"\n",
    "#     recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")  # Change to CE for text\n",
    "#     kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL Divergence\n",
    "#     return recon_loss + kl_div\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # === Generate a new batch of Markov transition matrices === #\n",
    "#     markov_matrices = torch.rand(batch_size, num_nodes, num_nodes)  # Example: Random transition matrices\n",
    "#     markov_matrices = markov_matrices / markov_matrices.sum(dim=-1, keepdim=True)  # Normalize rows\n",
    "\n",
    "#     # === Convert batch of matrices into a batched PyG graph === #\n",
    "#     batch_graph, node_indices = build_batch_graphs(markov_matrices)\n",
    "\n",
    "#     # === Generate Random Input Data (Replace with real input) === #\n",
    "#     input_ids = torch.randint(0, 1000, (batch_size, seq_length))  # Example tokenized input\n",
    "#     attention_mask = torch.ones_like(input_ids)  # Dummy attention mask\n",
    "\n",
    "#     # === Forward Pass (Now batch-processed) === #\n",
    "#     recon_x, mu, logvar = cvae(input_ids, attention_mask, batch_graph, node_indices)\n",
    "    \n",
    "#     total_loss = vae_loss(recon_x, input_ids, mu, logvar)\n",
    "#     total_loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch}, Loss: {total_loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
